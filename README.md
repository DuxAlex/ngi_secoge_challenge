# ğŸš€ NGI Secoge Challenge

Este projeto foi criado como parte do desafio **NGI Secoge**. Ele envolve a construÃ§Ã£o de uma pipeline de dados utilizando Python e SQL. O objetivo Ã© realizar a extraÃ§Ã£o, transformaÃ§Ã£o e carregamento de dados, alÃ©m de gerar relatÃ³rios.

## âš™ï¸ Ferramentas Utilizadas

- ğŸ **Python** â€“ Linguagem de programaÃ§Ã£o para manipulaÃ§Ã£o de dados e execuÃ§Ã£o do pipeline.
- ğŸ“Š **Pandas** â€“ Biblioteca para manipulaÃ§Ã£o e anÃ¡lise de dados.
- ğŸŒ **Requests** â€“ Biblioteca para realizar requisiÃ§Ãµes HTTP.
- ğŸ˜ **psycopg2** â€“ Adaptador PostgreSQL para Python, utilizado para comunicaÃ§Ã£o com o banco de dados.
- ğŸ›  **virtualenv** â€“ Ferramenta para criar ambientes virtuais em Python.
- âš¡ **SQLAlchemy** â€“ Biblioteca para trabalhar com bancos de dados SQL de forma mais eficiente.
- ğŸ‹ **Docker** â€“ Utilizado para rodar containers de ambiente e banco de dados.
- âœˆï¸ **Airflow** â€“ Plataforma para orquestraÃ§Ã£o de workflows e automaÃ§Ã£o de pipelines de dados.
- ğŸ˜ **PostgreSQL (Docker)** â€“ Banco de dados relacional utilizado no projeto.
- âš¡ **SQL** â€“ Para consultas e manipulaÃ§Ã£o dos dados.

## ğŸ“‚ Estrutura de DiretÃ³rios

```bash
.
â”œâ”€â”€ dags
â”‚Â Â  â”œâ”€â”€ descricao_diretorio_dags.txt               # DescriÃ§Ã£o sobre as DAGs utilizadas
â”‚Â Â  â”œâ”€â”€ direto_etl_dados_influd2024.py             # Script principal da DAG para o ETL
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ descricao_diretorio_data.txt               # InformaÃ§Ãµes sobre os dados armazenados
â”‚Â Â  â”œâ”€â”€ Dicionario_de_Dados_SRAG_Hospitalizado...  # DicionÃ¡rio de dados para referÃªncia
â”‚Â Â  â”œâ”€â”€ DicionÃ¡rio dos headers nescessÃ¡rios.txt    # Mapeamento dos headers essenciais
â”‚Â Â  â”œâ”€â”€ Info_adicionais.txt                        # InformaÃ§Ãµes adicionais sobre os dados
â”‚Â Â  â””â”€â”€ setup.sql                                  # Script SQL para inicializar o banco
â”œâ”€â”€ db
â”‚Â Â  â”œâ”€â”€ descricao_diretorio_data.txt               # DescriÃ§Ã£o sobre os arquivos de banco
â”‚Â Â  â”œâ”€â”€ obitos_f_ou_m.py                           # Script Python para anÃ¡lise de Ã³bitos
â”‚Â Â  â”œâ”€â”€ obitos_f_ou_m.sql                          # Consulta SQL para anÃ¡lise de Ã³bitos
â”‚Â Â  â”œâ”€â”€ obitos_menos_12_vacinad.sql                # Consulta SQL sobre vacinaÃ§Ã£o de menores
â”‚Â Â  â”œâ”€â”€ obitos_vacina.sql                          # Consulta SQL sobre Ã³bitos e vacinas
â”‚Â Â  â””â”€â”€ teste_conexao.py                           # Script para testar conexÃ£o com o banco
â”œâ”€â”€ docker-compose.yml                             # Arquivo de configuraÃ§Ã£o do Docker Compose
â”œâ”€â”€ dockerfile                                     # Dockerfile para construÃ§Ã£o da imagem
â”œâ”€â”€ help
â”‚Â Â  â”œâ”€â”€ desafio tÃ©cnico bgi secoge Eng de dados... # Documento detalhando o desafio tÃ©cnico
â”‚Â Â  â”œâ”€â”€ desafio tÃ©cnico bgi secoge Eng de dados... # VersÃ£o em texto do desafio tÃ©cnico
â”‚Â Â  â””â”€â”€ diagrama_projeto.png                       # Diagrama visual do projeto
â”œâ”€â”€ README.md                                      # Arquivo README principal do projeto
â”œâ”€â”€ RELATÃ“RIO.md                                   # RelatÃ³rio do projeto
â”œâ”€â”€ requirements.txt                               # DependÃªncias do projeto
â””â”€â”€ scripts
    â”œâ”€â”€ descricao_diretorio_scripts.txt            # InformaÃ§Ãµes sobre os scripts
    â”œâ”€â”€ extracao.py                                # Script para extraÃ§Ã£o de dados
    â”œâ”€â”€ main.py                                    # Script principal do projeto
    â””â”€â”€ transformar_salvar.py                      # Script para transformar e salvar dados

6 directories, 26 files
```
ğŸ› ï¸ ObservaÃ§Ãµes:
- Cada diretÃ³rio contÃ©m uma descriÃ§Ã£o detalhada para facilitar a navegaÃ§Ã£o e entendimento.
- Utilize os scripts localizados na pasta `db` para realizar consultas e validaÃ§Ãµes no banco de dados.

![Escopo do processos realizados no projeto](help/diagrama_ETL_ngisecoge.png)

## ğŸ“¥ Como Rodar o Projeto

### 1ï¸âƒ£ Clone o repositÃ³rio

Primeiro, faÃ§a o clone do repositÃ³rio:

```bash
git clone https://github.com/DuxAlex/ngi_secoge_challenge
```

### 2ï¸âƒ£ Verifique se o Docker estÃ¡ instalado

Certifique-se de que o Docker estÃ¡ instalado no seu computador. Para verificar, execute:

```
docker --version
```

Se nÃ£o estiver instalado, siga a [a documentaÃ§Ã£o do docker](https://docs.docker.com/engine/install/) para realizar a instalaÃ§Ã£o.

### 3ï¸âƒ£ Verifique se o Python estÃ¡ instalado

Verifique se o Python estÃ¡ instalado:

```
python --version
```

Se nÃ£o estiver instalado, siga a [a documentaÃ§Ã£o do python](https://www.python.org/downloads/) para realizar a instalaÃ§Ã£o.

## Ambiente Virtual

## 4ï¸âƒ£ Instale o `virtualenv`:

    ```bash
    pip install virtualenv
    ```

## 5ï¸âƒ£ **Crie o ambiente virtual**:

    ```bash
    virtualenv .venv
    ou
    python3 -m venv .venv

    ```

## 6ï¸âƒ£ **Ative o ambiente virtual**:

    - **No Linux**:
    
      ```bash
      source .venv/bin/activate
      ```
    
    - **No Windows**:
    
      ```bash
      venv\Scripts\activate
      ```

## 7ï¸âƒ£ **Instale as dependÃªncias do projeto**:

    ```bash
    pip install -r requirements.txt
    ```

## 8ï¸âƒ£ **Subindo o Docker Compose:**

Depois de executar os scripts, agora Ã© necessÃ¡rio subir os contÃªineres usando o Docker Compose. 

na raiz do diretÃ³rio digite:

```
se no linux:
sudo docker compose up -d

ou windows 
docker compose up -d

verifique a situaÃ§Ã£o dos containers digitando
sudo docker ps


e tambÃ©m:

sudo docker logs ngi_secoge_postgres
e
sudo docker logs ngi_secoge_airflow
```

## ExplicaÃ§Ã£o dos CÃ³digos

### 1ï¸âƒ£ `docker-compose.yml`

O arquivo `docker-compose.yml` define dois serviÃ§os principais: **PostgreSQL** e **Airflow**, configurados para rodar em contÃªineres Docker. 

#### ServiÃ§os:

- **PostgreSQL**:
  
  - Utiliza a imagem `postgres:12`.
  - O contÃªiner Ã© chamado `ngi_secoge_postgres`.
  - VariÃ¡veis de ambiente sÃ£o configuradas para criar o banco de dados (`ngisecoge`), o usuÃ¡rio e a senha.
  - Mapeia dois diretÃ³rios:
    - O diretÃ³rio `./dbdata` local para o diretÃ³rio de dados do PostgreSQL no contÃªiner.
    - O diretÃ³rio `./data` local para o diretÃ³rio de inicializaÃ§Ã£o do banco de dados.
  - A porta `5432` Ã© mapeada para permitir a comunicaÃ§Ã£o com o banco de dados.
  - EstÃ¡ na rede `ngi_secoge_net`.

- **Airflow**:
  
  - ConstrÃ³i a imagem do Airflow a partir do diretÃ³rio atual com base no `Dockerfile`.
  - O contÃªiner Ã© chamado `ngi_secoge_airflow`.
  - VariÃ¡veis de ambiente configuram a conexÃ£o com o banco de dados PostgreSQL, desativam os exemplos de DAGs e definem o executor como `SequentialExecutor`.
  - O serviÃ§o **Airflow** depende do **PostgreSQL**.
  - A execuÃ§Ã£o do Airflow no contÃªiner inicia o banco de dados, cria o usuÃ¡rio Admin, e inicia o webserver e o scheduler.
  - Mapeia diretÃ³rios locais para o contÃªiner, incluindo:
    - `./dags` para o diretÃ³rio de DAGs.
    - `./logs` para os logs do Airflow.
    - `./scripts` para scripts.
    - `./data` para dados.
    - `./plugins` para plugins personalizados.
    - `./requirements.txt:/opt/airflow/requirements.txt Mapeia o arquivo de requisitos`
  - A porta `8080` Ã© mapeada para acessar o Airflow Web UI.
  - EstÃ¡ na rede `ngi_secoge_net`.

#### Rede:

- A rede `ngi_secoge_net` Ã© criada com o driver `bridge` para permitir que os contÃªineres se comuniquem entre si.

---

### 2ï¸âƒ£ `Dockerfile` do Airflow

O `Dockerfile` personalizado do **Airflow** define as etapas para construir uma imagem do Docker com o Airflow configurado de forma personalizada. 

#### Passos:

- **Imagem Base**:
  
  - A imagem base utilizada Ã© `apache/airflow:latest-python3.8`.

- **InstalaÃ§Ã£o do `sudo`**:
  
  - O `sudo` Ã© instalado (se necessÃ¡rio) para permitir que o usuÃ¡rio `airflow` execute comandos como superusuÃ¡rio.

- **PermissÃ£o para usar `sudo` sem senha**:
  
  - Ã‰ configurado para que o usuÃ¡rio `airflow` possa executar o `sudo` sem a necessidade de fornecer uma senha, facilitando as execuÃ§Ãµes de comandos com permissÃµes elevadas dentro do contÃªiner.

- **InstalaÃ§Ã£o de DependÃªncias Adicionais **:
  
  - Instala as dependencias lista no arquivo `requirements.txt`.

- **ConfiguraÃ§Ã£o do DiretÃ³rio de Trabalho**:
  
  - O diretÃ³rio de trabalho Ã© configurado para `/opt/airflow`, que Ã© o local padrÃ£o onde o Airflow serÃ¡ executado.

- **DefiniÃ§Ã£o do UsuÃ¡rio**:
  
  - O usuÃ¡rio padrÃ£o no contÃªiner Ã© o `airflow`, o que garante que o Airflow seja executado com permissÃµes adequadas.

## ExplicaÃ§Ã£o do CÃ³digo setup.SQL na pasta data

### 1ï¸âƒ£ **Definir o formato da data**

```sql
SET datestyle = 'ISO, DMY';
```

Este comando define o formato da data para **ISO**, com a ordem **dia-mÃªs-ano** (DMY). Ele garante que as datas sejam interpretadas corretamente pelo PostgreSQL quando forem inseridas ou manipuladas.

### 2ï¸âƒ£ **CriaÃ§Ã£o da Tabela**

```sql
CREATE TABLE IF NOT EXISTS dados_influd24 (
 SG_UF_NOT VARCHAR(2),
 ID_MUNICIP VARCHAR(50),
 CO_MUN_NOT VARCHAR(7),
 ESTRANG VARCHAR(3),
 CS_SEXO VARCHAR(1),
 DT_NASC DATE,
 NU_IDADE_N VARCHAR(3),
 CS_GESTANT VARCHAR(1),
 CS_RACA VARCHAR(1),
 FATOR_RISC VARCHAR(1),
 VACINA_COV VARCHAR(1),
 CLASSI_FIN VARCHAR(1),
 EVOLUCAO VARCHAR(1)
);
```

Este trecho de cÃ³digo cria uma tabela chamada `dados_influd24`, se ela nÃ£o existir ainda. A tabela possui as seguintes colunas:

- **SG_UF_NOT**: Sigla do estado (2 caracteres).
- **ID_MUNICIP**: Identificador do municÃ­pio (mÃ¡ximo de 50 caracteres).
- **CO_MUN_NOT**: CÃ³digo do municÃ­pio (7 caracteres).
- **ESTRANG**: Indicador de estrangeiro (3 caracteres).
- **CS_SEXO**: Sexo (1 caractere).
- **DT_NASC**: Data de nascimento (formato de data).
- **NU_IDADE_N**: Idade (3 caracteres).
- **CS_GESTANT**: Indicador de gestante (1 caractere).
- **CS_RACA**: Cor ou raÃ§a (1 caractere).
- **FATOR_RISC**: Fator de risco (1 caractere).
- **VACINA_COV**: Indicador de vacinaÃ§Ã£o contra COVID-19 (1 caractere).
- **CLASSI_FIN**: ClassificaÃ§Ã£o final (1 caractere).
- **EVOLUCAO**: EvoluÃ§Ã£o (1 caractere).

### 3ï¸âƒ£ **ImportaÃ§Ã£o dos Dados CSV**

```sql
COPY dados_influd24(SG_UF_NOT, ID_MUNICIP, CO_MUN_NOT, ESTRANG, CS_SEXO, DT_NASC, NU_IDADE_N, CS_GESTANT, CS_RACA, FATOR_RISC, VACINA_COV, CLASSI_FIN, EVOLUCAO) 
FROM '/docker-entrypoint-initdb.d/transformado_INFLUD24-20-01-2025.csv' 
DELIMITER ';'
CSV HEADER;
```

Este comando **COPY** importa os dados de um arquivo CSV (`transformado_INFLUD24-20-01-2025.csv`) para a tabela `dados_influd24`.

##OBS:
**Entretanto**, a parte de carregamento dos dados transformados serÃ¡ feita atravÃ©s da DAG no Airflow. O arquivo `setup.sql` apenas criarÃ¡ o escopo da tabela.

- O arquivo CSV estÃ¡ localizado no diretÃ³rio `/docker-entrypoint-initdb.d/`, que Ã© onde o PostgreSQL busca os arquivos para importar quando o contÃªiner Ã© iniciado.
- O delimitador do arquivo CSV Ã© `;`, indicando que os campos no arquivo sÃ£o separados por ponto e vÃ­rgula.
- A opÃ§Ã£o `CSV HEADER` indica que a primeira linha do arquivo contÃ©m os nomes das colunas, que serÃ£o ignoradas durante a importaÃ§Ã£o.

---

Esses comandos sÃ£o usados para configurar o PostgreSQL e carregar os dados de um arquivo CSV na tabela criada. A tabela contÃ©m informaÃ§Ãµes relacionadas ao COVID-19 e vacinaÃ§Ã£o, e a importaÃ§Ã£o Ã© feita para anÃ¡lise e uso posterior no banco de dados.



## ğŸ”Œ Conectando ao Banco de Dados

Para facilitar a interaÃ§Ã£o com o banco de dados e realizar consultas SQL, recomendamos utilizar ferramentas como o **DBeaver** ou a extensÃ£o **MySQL** do VSCode, chamada **Weijan Chen** disponÃ­vel no [Marketplace do VSCode](https://marketplace.visualstudio.com/items?itemName=weijanchen.database-client).

### ğŸ”§ Usando o DBeaver:

1. Instale o DBeaver [aqui](https://dbeaver.io/download/).
2. Abra o DBeaver e crie uma nova conexÃ£o PostgreSQL.
3. Preencha os detalhes da conexÃ£o com as seguintes configuraÃ§Ãµes:
   - **Host**: `localhost` OU `127.0.0.1`
   - **Porta**: `5432`
   - **Banco de Dados**: `ngisecoge`
   - **UsuÃ¡rio**: `ngisecoge`
   - **Senha**: `ngisecoge`
4. Conecte-se ao banco de dados e comece a executar as consultas.

### ğŸ’» Usando a ExtensÃ£o MySQL no VSCode:

1. Instale a extensÃ£o **Database Client** de Weijan Chen no VSCode.
2. Adicione uma nova conexÃ£o no painel de conexÃµes.
3. Use as configuraÃ§Ãµes de conexÃ£o para o banco de dados PostgreSQL, da mesma forma que faria no DBeaver.
4. Acesse a pasta `db` para consultar os scripts SQL e execute as consultas diretamente no VSCode.

Essas ferramentas vÃ£o facilitar a visualizaÃ§Ã£o, manipulaÃ§Ã£o e execuÃ§Ã£o de consultas SQL diretamente no banco de dados.

---



## ğŸ“š InformaÃ§Ãµes Adicionais

Na pasta `help`, vocÃª encontrarÃ¡ uma sÃ©rie de informaÃ§Ãµes Ãºteis que podem te ajudar a compreender melhor os dados utilizados no projeto. Essas informaÃ§Ãµes incluem: 

- ğŸ“„ **DocumentaÃ§Ã£o sobre os dados**: Detalhes sobre a origem, estrutura e significado das colunas do arquivo CSV.

- ğŸ—ºï¸ **Fontes dos dados**: ExplicaÃ§Ãµes sobre de onde os dados foram extraÃ­dos e como foram tratados. 

- âš™ï¸ **OrientaÃ§Ãµes e exemplos**: Exemplos de uso e orientaÃ§Ãµes para facilitar a navegaÃ§Ã£o e execuÃ§Ã£o do projeto.

- ğŸ“‘ **ReferÃªncias**: Links e materiais adicionais que podem ser Ãºteis para entender o contexto e o processo de coleta dos dados.

Recomendamos dar uma olhada na pasta `help` para uma compreensÃ£o mais aprofundada do projeto e seus dados.

Agradecemos por acompanhar este processo e esperamos que o projeto tenha sido Ãºtil para demonstrar a integraÃ§Ã£o entre as ferramentas utilizadas.


## ğŸ“ **ConsideraÃ§Ãµes Finais**

Este projeto tem como objetivo demonstrar o processo de integraÃ§Ã£o entre o PostgreSQL, Airflow e a manipulaÃ§Ã£o de dados provenientes de um arquivo CSV. Abaixo estÃ£o as principais etapas e conclusÃµes do processo:

1ï¸âƒ£ **Estrutura do Banco de Dados**:  
A criaÃ§Ã£o e importaÃ§Ã£o dos dados para a tabela `dados_influd24` foi realizada com sucesso, permitindo a manipulaÃ§Ã£o e anÃ¡lise das informaÃ§Ãµes sobre vacinaÃ§Ã£o e dados de saÃºde.

2ï¸âƒ£ **AutomatizaÃ§Ã£o com Airflow**:  
A configuraÃ§Ã£o do Airflow no Docker foi realizada, garantindo a automaÃ§Ã£o do processo de ETL (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carregamento) com o uso de DAGs. O Airflow facilita a orquestraÃ§Ã£o das tarefas de processamento de dados e o monitoramento de sua execuÃ§Ã£o.

3ï¸âƒ£ **ExecuÃ§Ã£o de Scripts**:  
O script Python (`main.py`) foi implementado para rodar automaticamente os processos de extraÃ§Ã£o e transformaÃ§Ã£o dos dados. AlÃ©m disso, a utilizaÃ§Ã£o de Docker Compose proporcionou um ambiente controlado e isolado para a execuÃ§Ã£o do sistema.

4ï¸âƒ£ **Requisitos e DependÃªncias**:  
Todas as dependÃªncias foram corretamente instaladas com o `pip` e `docker-compose`, e a estrutura do projeto foi organizada de forma eficiente, permitindo a fÃ¡cil execuÃ§Ã£o e manutenÃ§Ã£o do sistema.

5ï¸âƒ£ **Melhorias e Feedbacks**:  
Embora o projeto esteja funcional, existem alguns pontos de instabilidade e oportunidades de melhoria no cÃ³digo. Estou aberto a feedbacks que possam contribuir para o aperfeiÃ§oamento do sistema.

ğŸ“§ Caso tenha sugestÃµes ou feedbacks, entre em contato:  

- **E-mail**: [alexkrypto.ti@gmail.com](mailto:alexkrypto.ti@gmail.com)  
- **LinkedIn**: [Alex Miqueias](https://www.linkedin.com/in/alexmiqueias/)

AgradeÃ§o por sua atenÃ§Ã£o e interesse neste projeto! ğŸ˜Š

---
